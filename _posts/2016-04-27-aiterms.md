---
layout: post_layout
title: 人工智能科普词
time: 2016年04月27日 星期三
published: true
excerpt_separator: "```"
---

# 人工智能科普词
## perceptron - 感知机

## ReLU
## cost function - 评价函数

## dropout

## batch normalization

## residual networks

## attention model
动态的将注意力集中到某些细节，提高识别性能。比如，看图说话图像理解，你可以根据一幅图生成一句话，很可能是非常宏观的。如果我们能够把注意力聚焦在这个机制的从引入到识别的过程中，根据目前的识别结果，动态一步一步调整聚焦到图像的细节，那么可以生成一些更合理或者更精细的表达，比如在图像中，关注一个飞碟，我们可以调整关注区域在图像中把飞碟的找出来，提取它的特征进行识别，得到图像的更准确的文字描述。



## reinforcement learning
在增强学习的框架中有两个部分，一部分是自主控制的单元（agent），一部分是环境（environment）。自主控制单元是通过选择不同的策略或者行为，希望能够最大化自己的长期预期收益，得到奖励；而环境将接收策略行为，修改状态，反馈出奖励。在这个增强学习的框架中有两个部分，一个部分是如何选择这些行为（policy function)，另外一部分是如何评价评估自己可能取得的这些收益（value function）。这个增强学习框架本身已经存在很多年了，和深度学习的结合就是指如何选择策略行为的函数，以及如何评估预期奖励的函数，由深度神经网络学习得到，例如AlphaGo围棋中的走棋网络（policy network）和评价网络（value networks）。


## cargo cult science - 草包族科学
在“草包族科学”下，你往往是复制了机器的表象，却没有深入理解机器背后的原理。或者，在航空领域，你制造飞机时会完全复制鸟类的样子，它的羽毛、翅膀等等。19世纪的人们很喜欢这么做，但取得的成就非常有限。

## Memory Network - 记忆存储网络 
经由存储器模块强化的深度学习帮助自然语言处理取得了令人印象深刻的结果。该系统基于这样的理念，即用连续向量描述词语和句子，经由深层架构的多层级完成对这些向量的转化，并将它们存储在一种联合型存储器里。这对问答和语言翻译都非常有效。

## recurrent neural net - 周期神经网络
在这些神经网络中，输出被反馈到输入端，这样你就能形成一个推理链。你可以借此来处序列信号，像语音、音频、视频和语言，初步结果相当不错。深度学习的下一个前沿课题是自然语言理解。
常规的前向神经网络有一个特点：你每次输入和输出是确定的关系，对于一副图像，无论何时输入进神经网络，我们一层一层计算后就会得到一个确定的结果，这是跟上下文不相关的。我们如何把记忆的能力引入到神经网络中去？最简单的一个思路是，在神经网络中加入一些状态，让它能记住一点事情。它的输出不仅取决于它的输入，也取决于它本身的状态。这是一个最基本的递归神经网络的思路。输出取决于本身的状态，我们也可以将其展开成一个时序系列的结构，就是说当前状态的输入不仅包括现在输入，也包含上一时刻的输出，这样就会构成一个非常深的网络。

## lstm - 长短时记忆网络
提出了一个记忆单元（memory cell）的概念，这个单元中加入了三个个门，一个输入门，一个输出门，一个遗忘门。输入门可以控制你的输入是否影响你的记忆当中的内容。输出门是影响你的记忆是否被输出出来影响将来。遗忘门是来看你的记忆是否自我更新保持下去。

## GRU 
简化成只有两个门，一个是更新门，一个重置门，控制记忆内容是否能继续保存下去

## Neural Turning Machine - 神经图灵机
有一个永久的的内存模块，有一个控制模块去控制如何根据输入去读取存储这些内存，并转换成输出。这个控制模块，可以用神经网络实现。

## 迁移学习
比方说这个是在深度学习的模型上，在上面这一部分是一个领域已经训练好的模型。那么在一个新的领域，如果这两个领域之间有某种联系、某种相似性的话，我们就不一定在新的领域需要那么多的数据来学习，你只需要一小部分。我们之所以能做到这一点是因为我们可以把大部分的模型给迁移过来，人有这种能力，但是我们在做这种数据迁移的过程中，我们一定要牢记把这种有偏的数据偏差给消除掉。

## bayesian program learning- 单个例学习
这是从一个例子就能学会，我们知道深度学习是有千万个例子的。实际上它用了我们过去没有涉及到的概念，就叫做结构，如果我们了解了一个问题的结构，那么这个结构的一个具体形式只用一个例子就可以学会了。其他的部分，需要很多例子的那一部分可能是参数、统计，这一部分我们实际上可以通过迁移学习来学习。

## Stochastic Gradient Descent - 随机梯度下降

## back-propagation 


    